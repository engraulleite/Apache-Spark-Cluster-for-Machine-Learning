# Apache-Spark-Cluster-for-Machine-Learning
  I set up a Spark cluster with Apache Hadoop HDFS for distributed processing and distributed storage. I used Docker to create a cluster with 5 machines:spark-master, spark-worker-1, spark-worker-2, spark-worker-3 and spark-history. YARN was also configured for an optimized resource management.
	The cluster had the objective of processing Machine Learning algorithms, more specifically, K-means,popular clustering algorithm in machine learning and data mining. A dataset was created with fictitious data.The algorithm searches for patterns and similarities between data in order to segment them into groups with similar characteristics. This analyse is widely used in Marketing projects.

## Services provided:
- [x] Spark cluster creation
- [x]	HDFS and YARN configuration at a Spark cluster
- [x] Machine learning algorithm processing
- [x]	Cluster monitoring with Hadoop YARN

## Developed Skills:
- [x] Apache Spark
- [x]	Apache Hadoop HDFS
- [x]	Apache Hadoop YARN
- [x]	PySpark
- [x]	K-means clustering algorithm
- [x]	Docker

## Architecture Diagram:

<img src="/Architecture Diagram.png">

I made this diagram using [Diagrams.net](https://app.diagrams.net/)
